{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "from typing import Literal\n",
    "\n",
    "import kagglehub  # pip install kagglehub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer  # pip install accelerate auto-gptq optimum huggingface_hub[hf_xet]\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "[View on Kaggle](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification)\n",
    "\n",
    "A **subset** of the original dataset was used for computational efficiency:\n",
    "\n",
    "- **4 classes**\n",
    "- 50 samples per class\n",
    "- **200 samples total**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  4\n",
      "Original: (50424, 2)\n",
      "Used: (200, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Clothing &amp; Accessories</td>\n",
       "      <td>Kuchipoo Girl's Cotton Regular Fit T-Shirt - P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Books</td>\n",
       "      <td>The Hindu View Of Life About the Author Profes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Household</td>\n",
       "      <td>Sehaz Artworks 'Our Memories' Pasted Wood Phot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Household</td>\n",
       "      <td>Paper Plane Design Starry Night Vangoh Wall Ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Clothing &amp; Accessories</td>\n",
       "      <td>FabSeasons Camouflage Polyester Multi Function...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label                                               text\n",
       "104  Clothing & Accessories  Kuchipoo Girl's Cotton Regular Fit T-Shirt - P...\n",
       "85                    Books  The Hindu View Of Life About the Author Profes...\n",
       "39                Household  Sehaz Artworks 'Our Memories' Pasted Wood Phot...\n",
       "5                 Household  Paper Plane Design Starry Night Vangoh Wall Ar...\n",
       "133  Clothing & Accessories  FabSeasons Camouflage Polyester Multi Function..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = kagglehub.dataset_download(\"saurabhshahane/ecommerce-text-classification\")\n",
    "csv_file = os.listdir(data_dir)[0]\n",
    "csv_path = os.path.join(data_dir, csv_file)\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data = data.rename(columns={\n",
    "                       data.columns[0]: \"label\",\n",
    "                       data.columns[1]: \"text\"\n",
    "                    })\n",
    "\n",
    "classes = data[\"label\"].unique()\n",
    "\n",
    "print(\"Labels: \", len(classes))\n",
    "print(f\"Original: {data.shape}\")\n",
    "\n",
    "# Keep only N examples of each category (N is a hyperparameter. Say, N=250)\n",
    "N = 50\n",
    "subset_data = pd.DataFrame()\n",
    "for label in classes:\n",
    "    label_data = data[data[\"label\"] == label].head(N)\n",
    "    subset_data = pd.concat([subset_data, label_data], ignore_index=True)\n",
    "\n",
    "data = subset_data.sample(frac=1)  # shuffle data\n",
    "print(f\"Used: {data.shape}\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `Qwen3-0.6B-GPTQ-Int8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B-GPTQ-Int8\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, shot_type: Literal[\"zero\", \"one\", \"few\"] = \"zero\", monte_carlo: bool = False, n_samples: int = 10):\n",
    "    \n",
    "    examples = \"\"\n",
    "    if shot_type == \"one\":\n",
    "        if not monte_carlo:\n",
    "            examples = \"\"\"\n",
    "Example:\n",
    "Product description: Samsung phone with 1.5 inch display, FM radio, and 800 mAh battery.\n",
    "Category: Electronics\n",
    "\n",
    "\"\"\"\n",
    "        else:\n",
    "            examples = \"\"\"\n",
    "Example:\n",
    "Product description: Samsung phone with 1.5 inch display, FM radio, and 800 mAh battery.\n",
    "Scores: {\"Household\": 1, \"Books\": 0, \"Clothing & Accessories\": 0, \"Electronics\": 10}\n",
    "\n",
    "\"\"\"\n",
    "    elif shot_type == \"few\":\n",
    "        if not monte_carlo:\n",
    "            examples = \"\"\"\n",
    "=== Examples start ===\n",
    "\n",
    "Product description: Canvas - 24x18\" ready-to-hang wall art with vibrant colors.\n",
    "Category: Household\n",
    "\n",
    "Product description: Mystery novel with 300 pages by Author Agatha Christie.\n",
    "Category: Books\n",
    "\n",
    "Product description: Wool sweater with v-neck.\n",
    "Category: Clothing & Accessories\n",
    "\n",
    "Product description: Samsung phone with 1.5 inch display, FM radio, and 800 mAh battery.\n",
    "Category: Electronics\n",
    "\n",
    "=== Examples end ===\n",
    "\n",
    "\"\"\"\n",
    "        else:\n",
    "            examples = \"\"\"\n",
    "Examples:\n",
    "\n",
    "Product description: Canvas - 24x18\" ready-to-hang wall art with vibrant colors.\n",
    "Scores: {\"Household\": 10, \"Books\": 0, \"Clothing & Accessories\": 1, \"Electronics\": 2}\n",
    "\n",
    "Product description: Mystery novel with 300 pages by Author Agatha Christie.\n",
    "Scores: {\"Household\": 1, \"Books\": 10, \"Clothing & Accessories\": 0, \"Electronics\": 0}\n",
    "\n",
    "Product description: Wool sweater with v-neck.\n",
    "Scores: {\"Household\": 0, \"Books\": 0, \"Clothing & Accessories\": 10, \"Electronics\": 0}\n",
    "\n",
    "Product description: Samsung phone with 1.5 inch display, FM radio, and 800 mAh battery.\n",
    "Scores: {\"Household\": 0, \"Books\": 0, \"Clothing & Accessories\": 1, \"Electronics\": 10}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    if not monte_carlo:\n",
    "        system_prompt = \"\"\"You are a product categorization expert. Your task is to classify product descriptions into exactly one of these categories:\n",
    "        - Household: Home items, tools, and outdoor equipment\n",
    "        - Books: Books, publications, and literature\n",
    "        - Clothing & Accessories: Apparel, fashion items, and accessories\n",
    "        - Electronics: Phones, devices, and electronic equipment\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "        {examples}Analyze the following product description carefully and classify it into the most appropriate category.\n",
    "        Return ONLY the category name without any explanation or additional text.\n",
    "\n",
    "        Product description: {text}\n",
    "        Category:\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # conduct text completion\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=30,\n",
    "            temperature=0.7,  # Added temperature to reduce deterministic behavior\n",
    "            top_p=0.9         # Added top_p sampling to improve diversity\n",
    "        )\n",
    "        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "        content = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "        # print(content)\n",
    "        return content\n",
    "    else:\n",
    "        # Monte Carlo approach\n",
    "        categories = [\"Household\", \"Books\", \"Clothing & Accessories\", \"Electronics\"]\n",
    "        results = {category: 0 for category in categories}\n",
    "        \n",
    "        system_prompt = \"\"\"You are a product categorization expert. Your task is to assign a confidence score from 0 to 10 for EACH of these categories:\n",
    "        - Household: Home items, tools, and outdoor equipment\n",
    "        - Books: Books, publications, and literature\n",
    "        - Clothing & Accessories: Apparel, fashion items, and accessories\n",
    "        - Electronics: Phones, devices, and electronic equipment\"\"\"\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            seed = np.random.randint(0, 10000)\n",
    "            temperature = np.random.uniform(0.5, 1.5)\n",
    "            top_k = np.random.randint(5, 50)\n",
    "\n",
    "            user_prompt = f\"\"\"\n",
    "            {examples}Analyze the following product description carefully and assign a confidence score from 0 to 10 for EACH category.\n",
    "            The highest score means the most probable class.\n",
    "            Return your answer as a dictionary with categories as keys and scores as values.\n",
    "            Format your response as: {{\"Household\": score, \"Books\": score, \"Clothing & Accessories\": score, \"Electronics\": score}}\n",
    "            Do not include any other text, explanations, or labels.\n",
    "\n",
    "            Product description: {text}\n",
    "            Scores:\"\"\"\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "\n",
    "            text_input = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=False\n",
    "            )\n",
    "            model_inputs = tokenizer([text_input], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "            # Set seed for reproducibility within this iteration\n",
    "            torch.manual_seed(seed)\n",
    "            \n",
    "            # Generate with varied parameters\n",
    "            generated_ids = model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=100,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                do_sample=True\n",
    "            )\n",
    "            output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "            content = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "            # print(content)\n",
    "\n",
    "            # Parse the dictionary output\n",
    "            try:\n",
    "                # Extract dictionary part from the response\n",
    "                dict_str = content.strip()\n",
    "                scores_dict = eval(dict_str)\n",
    "                # print(scores_dict)\n",
    "                \n",
    "                # Normalize scores to create a probability distribution\n",
    "                total = sum(scores_dict.values())\n",
    "                for category in categories:\n",
    "                    if category in scores_dict:\n",
    "                        results[category] += scores_dict[category] / total\n",
    "            except:\n",
    "                # If parsing fails, use equal weights\n",
    "                for category in categories:\n",
    "                    results[category] += 0.25\n",
    "\n",
    "        # Find the category with the highest aggregated score\n",
    "        max_category = max(results, key=results.get)\n",
    "        return max_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(use_mc: bool = False):\n",
    "    results = {}\n",
    "\n",
    "    for shot in [\"few\", \"zero\", \"one\"]:\n",
    "        n_correct = 0\n",
    "        n_total = 0\n",
    "        \n",
    "        # Create a progress bar\n",
    "        pbar = tqdm(zip(data[\"text\"].values, data[\"label\"].values), total=data.shape[0], \n",
    "                   desc=f\"{'w/ MC' if use_mc else 'w/o MC'}, {shot} shot\")\n",
    "        \n",
    "        for text, label in pbar:\n",
    "            # Make prediction, update counters\n",
    "            prediction = classify_text(text, shot_type=shot, monte_carlo=use_mc)\n",
    "            n_total += 1\n",
    "            if prediction.lower() == label.lower():\n",
    "                n_correct += 1\n",
    "            \n",
    "            # Update progress bar with current accuracy\n",
    "            current_accuracy = n_correct / n_total\n",
    "            pbar.set_description(f\"{'w/ MC' if use_mc else 'w/o MC'}, {shot} shot - Acc: {current_accuracy:.4f}\")\n",
    "\n",
    "        accuracy = n_correct / n_total\n",
    "        results[shot] = {\"correct\": n_correct, \"total\": n_total, \"accuracy\": accuracy}\n",
    "        # print(f\"{shot}-shot accuracy: {accuracy:.4f} ({n_correct}/{n_total})\")\n",
    "\n",
    "    # Create and display a table with results\n",
    "    table_data = []\n",
    "    for shot in [\"zero\", \"one\", \"few\"]:\n",
    "        table_data.append([\n",
    "            f\"{shot}-shot\",\n",
    "            results[shot][\"correct\"],\n",
    "            results[shot][\"total\"],\n",
    "            f\"{results[shot]['accuracy']:.4f}\"\n",
    "        ])\n",
    "\n",
    "    print(\"\\nAccuracy Results:\")\n",
    "    print(tabulate(table_data, headers=[\"Shot Type\", \"Correct\", \"Total\", \"Accuracy\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w/o MC, few shot - Acc: 0.5150: 100%|██████████| 200/200 [00:34<00:00,  5.84it/s]\n",
      "w/o MC, zero shot - Acc: 0.5050: 100%|██████████| 200/200 [00:30<00:00,  6.57it/s]\n",
      "w/o MC, one shot - Acc: 0.5800: 100%|██████████| 200/200 [00:32<00:00,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Results:\n",
      "+-------------+-----------+---------+------------+\n",
      "| Shot Type   |   Correct |   Total |   Accuracy |\n",
      "+=============+===========+=========+============+\n",
      "| zero-shot   |       101 |     200 |      0.505 |\n",
      "+-------------+-----------+---------+------------+\n",
      "| one-shot    |       116 |     200 |      0.58  |\n",
      "+-------------+-----------+---------+------------+\n",
      "| few-shot    |       103 |     200 |      0.515 |\n",
      "+-------------+-----------+---------+------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_experiments(use_mc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w/ MC, few shot - Acc: 0.6300: 100%|██████████| 200/200 [53:53<00:00, 16.17s/it]\n",
      "w/ MC, zero shot - Acc: 0.6000: 100%|██████████| 200/200 [53:41<00:00, 16.11s/it]\n",
      "w/ MC, one shot - Acc: 0.4700: 100%|██████████| 200/200 [53:27<00:00, 16.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Results:\n",
      "+-------------+-----------+---------+------------+\n",
      "| Shot Type   |   Correct |   Total |   Accuracy |\n",
      "+=============+===========+=========+============+\n",
      "| zero-shot   |       120 |     200 |       0.6  |\n",
      "+-------------+-----------+---------+------------+\n",
      "| one-shot    |        94 |     200 |       0.47 |\n",
      "+-------------+-----------+---------+------------+\n",
      "| few-shot    |       126 |     200 |       0.63 |\n",
      "+-------------+-----------+---------+------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_experiments(use_mc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "- The Monte Carlo method demonstrated a significant improvement **(9.5–11.5%)**, except in the one-shot prompting scenario.\n",
    "\n",
    "- Few-shot and one-shot prompting **outperformed** zero-shot prompting in 3 out of 4 cases.\n",
    "\n",
    "- The anomaly where one-shot prompting underperforms zero-shot in the Monte Carlo setting is likely due to the **small dataset size** and **poor examples**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
