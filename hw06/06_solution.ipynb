{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a570f920",
   "metadata": {},
   "source": [
    "# Homework assignment №6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9924a33",
   "metadata": {},
   "source": [
    "# Music Generation (with xLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a97f6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "# conda env create --name xlstm -f environment_pt260cu126.yaml\n",
    "# conda activate xlstm\n",
    "# pip install xlstm mido midi2audio\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mido import Message, MidiFile, MidiTrack\n",
    "from midi2audio import FluidSynth\n",
    "from IPython.display import Audio\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from dacite import from_dict\n",
    "from dacite import Config as DaciteConfig\n",
    "from xlstm import xLSTMLMModel, xLSTMLMModelConfig\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "match device.type:\n",
    "    case \"cuda\":\n",
    "        print(f\"Device: {torch.cuda.get_device_name()}\")\n",
    "    case \"cpu\":\n",
    "        print(\"Device: CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad98e33",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "`chopin-notes` is a custom dataset that I collected a year ago for a side project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a8daf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 4289\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/chopin-notes\", \"rb\") as f:\n",
    "    notes: list[str] = pickle.load(f)\n",
    "\n",
    "unique_notes = list(set(notes))\n",
    "vocab_size = len(unique_notes)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "note_to_idx = {note: i for i, note in enumerate(unique_notes)}\n",
    "idx_to_note = {i: note for note, i in note_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "437dd02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, B: int, L: int):\n",
    "        \"\"\"\n",
    "        Dataloader to parse data in a format needed to train Large Language models.\n",
    "        \"\"\"\n",
    "        self.B = B\n",
    "        self.L = L\n",
    "\n",
    "        ids = list(map(note_to_idx.get, notes))\n",
    "        ids = torch.Tensor(ids).type(torch.long)\n",
    "\n",
    "        self.tokens = ids\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, L = self.B, self.L\n",
    "        buf = self.tokens[self.current_position : self.current_position + B * L + 1]\n",
    "        inputs = (buf[:-1]).view(B, L)\n",
    "        targets = (buf[1:]).view(B, L)\n",
    "        self.current_position += B * L\n",
    "\n",
    "        if self.current_position + B * L < len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e2e55",
   "metadata": {},
   "source": [
    "# Set up and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ce96a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 0.15M\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "cfg = OmegaConf.load(\"xlstm_config.yaml\")\n",
    "cfg = from_dict(data_class=xLSTMLMModelConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))\n",
    "model = xLSTMLMModel(cfg)\n",
    "\n",
    "# Print the size of the model (a number of parameters it has) in millions\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total Parameters: {num_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de04231b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:14<00:00, 14.84it/s]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE: int = 32\n",
    "CONTEXT_WINDOW: int = 512\n",
    "\n",
    "dataloader = DataLoader(B=BATCH_SIZE, L=CONTEXT_WINDOW)\n",
    "\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, betas=(0.9, 0.95), eps=1e-8, fused=True)\n",
    "scheduler = CosineAnnealingLR(optimizer=optimizer, eta_min=1e-4, T_max=1000)\n",
    "\n",
    "with open(\"training_logs.csv\", mode=\"w\", newline=\"\", encoding=\"utf8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Step\", \"Loss\", \"Norm\", \"LR\", \"dt (ms)\", \"Tokens/sec\"])\n",
    "\n",
    "for step in tqdm(range(0, 2000)):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    inputs, targets = dataloader.next_batch()\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "    loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    tokens_processed =  dataloader.B * dataloader.L\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "    with open(\"training_logs.csv\", mode=\"a\", newline=\"\", encoding=\"utf8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([step, f\"{loss.item():.6f}\", f\"{norm:.4f}\", f\"{scheduler.get_last_lr()[0]:.4e}\", f\"{dt * 1000:.2f}\", f\"{tokens_per_sec:.2f}\"])\n",
    "\n",
    "torch.save(model.state_dict(), f\"xlstm_cp_{step}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e697887",
   "metadata": {},
   "source": [
    "### Let's generate a song using the last `context_window` notes from the dataset\n",
    "(the last sequence is used, because the model doesn't know how it continues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "986206c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, notes_to_generate: int = 32, prompt: list[str] = \"\"):\n",
    "    model.eval()\n",
    "    ids = list(map(note_to_idx.get, prompt))\n",
    "    ids = torch.Tensor(ids).type(torch.long).to(device).unsqueeze(0)\n",
    "    generated_tokens = []\n",
    "\n",
    "    for _ in range(notes_to_generate):\n",
    "        with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            logits = model(ids)[:, -1, :]\n",
    "        next_token = torch.multinomial(F.softmax(logits, dim=-1), 1)\n",
    "        generated_tokens.append(next_token.item())\n",
    "        prompt = torch.cat((ids[:, 1:], torch.multinomial(F.softmax(logits, dim=-1), 1)), dim=1)\n",
    "\n",
    "    return list(map(idx_to_note.get, generated_tokens))\n",
    "\n",
    "generated_song = generate_text(model=model, notes_to_generate=512, prompt=notes[-CONTEXT_WINDOW:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2964fdf",
   "metadata": {},
   "source": [
    "### Convert the generated notes (`str` objects) to the `.mid` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04ee4b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE_MAPPING = {\n",
    "    'C': 0, 'C#': 1, 'D': 2, 'D#': 3, 'E': 4, 'F': 5,\n",
    "    'F#': 6, 'G': 7, 'G#': 8, 'A': 9, 'A#': 10, 'B': 11\n",
    "}\n",
    "\n",
    "def note_to_midi(note):\n",
    "    if note[-1].isdigit():\n",
    "        pitch, octave = note[:-1], int(note[-1])\n",
    "        return (octave + 1) * 12 + NOTE_MAPPING[pitch]\n",
    "    return None\n",
    "\n",
    "midi = MidiFile()\n",
    "track = MidiTrack()\n",
    "midi.tracks.append(track)\n",
    "tempo = 120  # BPM\n",
    "tick_duration = midi.ticks_per_beat // 4  # 16th note\n",
    "\n",
    "for note_group in generated_song:\n",
    "    note_list = note_group.split('.')\n",
    "    midi_numbers = [note_to_midi(note) for note in note_list if note_to_midi(note) is not None]\n",
    "    for midi_num in midi_numbers:\n",
    "        track.append(Message('note_on', note=midi_num, velocity=64, time=0))\n",
    "    for midi_num in midi_numbers:\n",
    "        track.append(Message('note_off', note=midi_num, velocity=64, time=tick_duration))\n",
    "\n",
    "midi_path = \"generated_song.mid\"\n",
    "midi.save(midi_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae52e7",
   "metadata": {},
   "source": [
    "## The file was converted to `.mp3` and attached to this repo.\n",
    "\n",
    "# Listen to it -> `hw06/README.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca9ea73",
   "metadata": {},
   "source": [
    "Немного выводов/заметок:\n",
    "\n",
    "1. Модель очень быстро переобучается, `embedding_dim >= 64` (300к+ параметров) приводит к нулевому лоссу.\n",
    "2. При маленьком контекстом окне (до 128) модель выучивает генерить одну ноту (застривает в локальном минимуме).\n",
    "3. Увеличение контекстного окна значительно улучшило генерацию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f88804",
   "metadata": {},
   "source": [
    "# Mamba VS xLSTM\n",
    "\n",
    "| Feature/Aspect                       |  xLSTM                                   |  Mamba                               |\n",
    "|--------------------------------------|------------------------------------------|--------------------------------------|\n",
    "| **Sound Smoothness**                 | ❌ Less smooth                          | ✅ More smooth                       |\n",
    "| **Note Diversity**                   | ❌ Limited range of notes               | ✅ More diverse note generation      |\n",
    "| **Ease of Hyperparameter Tuning**    | ❌ Requires meticulous tuning           | ✅ Easier to set up                  |\n",
    "| **Training Stability**               | ✅ Stable                               | ✅ Stable                            |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
